{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # BeautifulSoup is in bs4 package \n",
    "import requests\n",
    "import re # Regular expressions\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "from nltk.corpus import stopwords # Filter out stopwords, such as 'the', 'or', 'and'\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(website):\n",
    "    '''\n",
    "    This function just cleans up the raw html so that I can look at it.\n",
    "    Inputs: a URL to investigate\n",
    "    Outputs: Cleaned text only\n",
    "    '''\n",
    "    try:\n",
    "        content = requests.get(website) \n",
    "    except: \n",
    "        return   # Need this in case the website isn't there anymore or some other weird connection problem \n",
    "    \n",
    "    soup_obj = BeautifulSoup(content.text, 'html.parser') # Get the html from the site\n",
    "    \n",
    "    for script in soup_obj([\"script\", \"style\"]):\n",
    "        script.extract() # Remove these two elements from the BS4 object\n",
    "    \n",
    "\n",
    "    text = soup_obj.get_text() # Get the text from this\n",
    "    \n",
    "        \n",
    "    \n",
    "    lines = (line.strip() for line in text.splitlines()) # break into lines\n",
    "    \n",
    "        \n",
    "        \n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \")) # break multi-headlines into a line each\n",
    "    \n",
    "    def chunk_space(chunk):\n",
    "        chunk_out = chunk + ' ' # Need to fix spacing issue\n",
    "        return chunk_out  \n",
    "        \n",
    "    \n",
    "    text = ''.join(chunk_space(chunk) for chunk in chunks if chunk).encode('utf-8') # Get rid of all blank lines and ends of line\n",
    "        \n",
    "        \n",
    "    # Now clean out all of the unicode junk (this line works great!!!)\n",
    "        \n",
    "    try:\n",
    "        text = text.decode('unicode_escape') # Need this as some websites aren't formatted\n",
    "    except:                                                            # in a way that this works, can occasionally throw\n",
    "        return                                                         # an exception\n",
    "       \n",
    "      \n",
    "    text = re.sub(\"[^a-zA-Z.+3#]\",\" \", text)  # Now get rid of any terms that aren't words (include 3 for d3.js)\n",
    "                                                # Also include + for C++\n",
    "        \n",
    "       \n",
    "    text = text.lower().split()  # Go to lower case and split them apart   \n",
    "        \n",
    "        \n",
    "    text = list(set(text)) # Last, just get the set of these. Ignore counts (we are just looking at whether a term existed\n",
    "                            # or not on the website)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = text_cleaner('http://www.indeed.com/viewjob?jk=5505e59f8e5a32a4&q=%22data+scientist%22&tk=19ftfgsmj19ti0l3&from=web&advn=1855944161169178&sjdu=QwrRXKrqZ3CNX5W-O9jEvWC1RT2wMYkGnZrqGdrncbKqQ7uwTLXzT1_ME9WQ4M-7om7mrHAlvyJT8cA_14IV5w&pub=pub-indeed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['be',\n",
       " 'you',\n",
       " 'requested',\n",
       " 'found',\n",
       " 'found.',\n",
       " 'page',\n",
       " 'again',\n",
       " 'mobile',\n",
       " 'not',\n",
       " 'try',\n",
       " 'could',\n",
       " 'home',\n",
       " 'the',\n",
       " 'indeed']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skills_info(city = None, state = None):\n",
    "    '''\n",
    "    This function will take a desired city/state and look for all new job postings\n",
    "    on Indeed.com. It will crawl all of the job postings and keep track of how many\n",
    "    use a preset list of typical data science skills. The final percentage for each skill\n",
    "    is then displayed at the end of the collation. \n",
    "        \n",
    "    Inputs: The location's city and state. These are optional. If no city/state is input, \n",
    "    the function will assume a national search (this can take a while!!!).\n",
    "    Input the city/state as strings, such as skills_info('Chicago', 'IL').\n",
    "    Use a two letter abbreviation for the state.\n",
    "    \n",
    "    Output: A bar chart showing the most commonly desired skills in the job market for \n",
    "    a data scientist. \n",
    "    '''\n",
    "        \n",
    "    final_job = 'data scientist' # searching for data scientist exact fit(\"data scientist\" on Indeed search)\n",
    "    \n",
    "    # Make sure the city specified works properly if it has more than one word (such as San Francisco)\n",
    "    if city is not None:\n",
    "        final_city = city.split() \n",
    "        final_city = '+'.join(word for word in final_city)\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=', final_job, '&l=', final_city,\n",
    "                    '%2C+', state] # Join all of our strings together so that indeed will search correctly\n",
    "    else:\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=\"', final_job, '\"']\n",
    "\n",
    "    final_site = ''.join(final_site_list) # Merge the html address together into one string\n",
    "\n",
    "    \n",
    "    base_url = 'http://www.indeed.com'\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        html = requests.get(final_site) # Open up the front page of our search first\n",
    "    except:\n",
    "        'That city/state combination did not have any jobs. Exiting . . .' # In case the city is invalid\n",
    "        return\n",
    "    soup = BeautifulSoup(html.text, 'html.parser') # Get the html from the first page\n",
    "    \n",
    "    # Now find out how many jobs there were\n",
    "    \n",
    "    num_jobs_area = soup.find(id = 'searchCount').encode('utf-8').decode('unicode_escape') # Now extract the total number of jobs found\n",
    "                                                                        # The 'searchCount' object has this\n",
    "    total_num_jobs = re.findall('of\\s(\\d\\d,?\\d+)', num_jobs_area)[0] # Extract the total jobs found from the search result\n",
    "  \n",
    "    print(total_num_jobs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387\n"
     ]
    }
   ],
   "source": [
    "skills_info(city = 'New York', state = 'NY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skills_info(city = None, state = None):\n",
    "    '''\n",
    "    This function will take a desired city/state and look for all new job postings\n",
    "    on Indeed.com. It will crawl all of the job postings and keep track of how many\n",
    "    use a preset list of typical data science skills. The final percentage for each skill\n",
    "    is then displayed at the end of the collation. \n",
    "        \n",
    "    Inputs: The location's city and state. These are optional. If no city/state is input, \n",
    "    the function will assume a national search (this can take a while!!!).\n",
    "    Input the city/state as strings, such as skills_info('Chicago', 'IL').\n",
    "    Use a two letter abbreviation for the state.\n",
    "    \n",
    "    Output: A bar chart showing the most commonly desired skills in the job market for \n",
    "    a data scientist. \n",
    "    '''\n",
    "        \n",
    "    final_job = 'data scientist' # searching for data scientist exact fit(\"data scientist\" on Indeed search)\n",
    "    \n",
    "    # Make sure the city specified works properly if it has more than one word (such as San Francisco)\n",
    "    if city is not None:\n",
    "        final_city = city.split() \n",
    "        final_city = '+'.join(word for word in final_city)\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=', final_job, '&l=', final_city,\n",
    "                    '%2C+', state] # Join all of our strings together so that indeed will search correctly\n",
    "    else:\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=\"', final_job, '\"']\n",
    "\n",
    "    final_site = ''.join(final_site_list) # Merge the html address together into one string\n",
    "\n",
    "    \n",
    "    base_url = 'http://www.indeed.com'\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        html = requests.get(final_site) # Open up the front page of our search first\n",
    "    except:\n",
    "        print('That city/state combination did not have any jobs. Exiting . . .') # In case the city is invalid\n",
    "        return\n",
    "    soup = BeautifulSoup(html.text, 'html.parser') # Get the html from the first page\n",
    "    \n",
    "    # Now find out how many jobs there were\n",
    "    \n",
    "    num_jobs_area = soup.find(id = 'searchCount').encode('utf-8').decode('unicode_escape') # Now extract the total number of jobs found\n",
    "                                                                        # The 'searchCount' object has this\n",
    "    total_num_jobs = re.findall('of\\s(\\d\\d,?\\d+)', num_jobs_area)[0] # Extract the total jobs found from the search result\n",
    "  \n",
    "    print(total_num_jobs)\n",
    "    \n",
    "    city_title = city\n",
    "    if city is None:\n",
    "        city_title = 'Nationwide'\n",
    "        \n",
    "    print('There were', total_num_jobs, 'jobs found in', city_title + ',' + state) # Display how many jobs were found\n",
    "    \n",
    "    num_pages = int(int(total_num_jobs)/10) # This will be how we know the number of times we need to iterate over each new\n",
    "                                      # search result page\n",
    "    job_descriptions = [] # Store all our descriptions in this list\n",
    "    \n",
    "    for i in range(1,num_pages+1): # Loop through all of our search result pages\n",
    "        print('Getting page', i)\n",
    "        start_num = str(i*10) # Assign the multiplier of 10 to view the pages we want\n",
    "        current_page = ''.join([final_site, '&start=', start_num])\n",
    "        # Now that we can view the correct 10 job returns, start collecting the text samples from each\n",
    "            \n",
    "        html_page = requests.get(current_page) # Get the page\n",
    "        soup = BeautifulSoup(html_page.text, 'html.parser')\n",
    "        \n",
    "        page_obj = soup # Locate all of the job links\n",
    "        job_link_area = page_obj.find(id = \"resultsCol\") # The center column on the page where the job postings exist\n",
    "        \n",
    "        job_links = job_link_area.find_all('a')\n",
    "        \n",
    "        for link in job_links:\n",
    "            job_hyperlinks = link.get('href')\n",
    "            \n",
    "        \n",
    "        job_related_URLS = []\n",
    "        for hyperlink in job_hyperlinks:\n",
    "            if 'clk' in str(hyperlink) == True:\n",
    "                job_related_URLS.append(hyperlink)\n",
    "            else:\n",
    "                pass\n",
    "        print(job_related_URLS)\n",
    "        \n",
    "       \n",
    "        \n",
    "     \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475\n",
      "There were 475 jobs found in New York,NY\n",
      "Getting page 1\n",
      "[]\n",
      "Getting page 2\n",
      "[]\n",
      "Getting page 3\n",
      "[]\n",
      "Getting page 4\n",
      "[]\n",
      "Getting page 5\n",
      "[]\n",
      "Getting page 6\n",
      "[]\n",
      "Getting page 7\n",
      "[]\n",
      "Getting page 8\n",
      "[]\n",
      "Getting page 9\n",
      "[]\n",
      "Getting page 10\n",
      "[]\n",
      "Getting page 11\n",
      "[]\n",
      "Getting page 12\n",
      "[]\n",
      "Getting page 13\n",
      "[]\n",
      "Getting page 14\n",
      "[]\n",
      "Getting page 15\n",
      "[]\n",
      "Getting page 16\n",
      "[]\n",
      "Getting page 17\n",
      "[]\n",
      "Getting page 18\n",
      "[]\n",
      "Getting page 19\n",
      "[]\n",
      "Getting page 20\n",
      "[]\n",
      "Getting page 21\n",
      "[]\n",
      "Getting page 22\n",
      "[]\n",
      "Getting page 23\n",
      "[]\n",
      "Getting page 24\n",
      "[]\n",
      "Getting page 25\n",
      "[]\n",
      "Getting page 26\n",
      "[]\n",
      "Getting page 27\n",
      "[]\n",
      "Getting page 28\n",
      "[]\n",
      "Getting page 29\n",
      "[]\n",
      "Getting page 30\n",
      "[]\n",
      "Getting page 31\n",
      "[]\n",
      "Getting page 32\n",
      "[]\n",
      "Getting page 33\n",
      "[]\n",
      "Getting page 34\n",
      "[]\n",
      "Getting page 35\n",
      "[]\n",
      "Getting page 36\n",
      "[]\n",
      "Getting page 37\n",
      "[]\n",
      "Getting page 38\n",
      "[]\n",
      "Getting page 39\n",
      "[]\n",
      "Getting page 40\n",
      "[]\n",
      "Getting page 41\n",
      "[]\n",
      "Getting page 42\n",
      "[]\n",
      "Getting page 43\n",
      "[]\n",
      "Getting page 44\n",
      "[]\n",
      "Getting page 45\n",
      "[]\n",
      "Getting page 46\n",
      "[]\n",
      "Getting page 47\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "skills_info(city = 'New York', state = 'NY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
