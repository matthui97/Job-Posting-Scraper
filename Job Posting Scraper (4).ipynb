{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # BeautifulSoup is in bs4 package \n",
    "import requests\n",
    "import re # Regular expressions\n",
    "from collections import Counter # Keep track of our term counts\n",
    "from nltk.corpus import stopwords # Filter out stopwords, such as 'the', 'or', 'and'\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(website):\n",
    "    '''\n",
    "    This function just cleans up the raw html so that I can look at it.\n",
    "    Inputs: a URL to investigate\n",
    "    Outputs: Cleaned text only\n",
    "    '''\n",
    "    try:\n",
    "        content = requests.get(website) \n",
    "    except: \n",
    "        return   # Need this in case the website isn't there anymore or some other weird connection problem \n",
    "    \n",
    "    soup_obj = BeautifulSoup(content.text, 'html.parser') # Get the html from the site\n",
    "    \n",
    "    for script in soup_obj([\"script\", \"style\"]):\n",
    "        script.extract() # Remove these two elements from the BS4 object\n",
    "    \n",
    "\n",
    "    text = soup_obj.get_text() # Get the text from this\n",
    "    \n",
    "        \n",
    "    \n",
    "    lines = (line.strip() for line in text.splitlines()) # break into lines\n",
    "    \n",
    "        \n",
    "        \n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \")) # break multi-headlines into a line each\n",
    "    \n",
    "    def chunk_space(chunk):\n",
    "        chunk_out = chunk + ' ' # Need to fix spacing issue\n",
    "        return chunk_out  \n",
    "        \n",
    "    \n",
    "    text = ''.join(chunk_space(chunk) for chunk in chunks if chunk).encode('utf-8') # Get rid of all blank lines and ends of line\n",
    "        \n",
    "        \n",
    "    # Now clean out all of the unicode junk (this line works great!!!)\n",
    "        \n",
    "    try:\n",
    "        text = text.decode('unicode_escape') # Need this as some websites aren't formatted\n",
    "    except:                                                            # in a way that this works, can occasionally throw\n",
    "        return                                                         # an exception\n",
    "       \n",
    "      \n",
    "    text = re.sub(\"[^(a-zA-Z\\\\a-zA-Z)][^a-zA-Z.+3#]\",\" \", text)  # Now get rid of any terms that aren't words (include 3 for d3.js)\n",
    "                                                # Also include + for C++\n",
    "        \n",
    "       \n",
    "    text = text.lower().split()  # Go to lower case and split them apart   \n",
    "        \n",
    "        \n",
    "    text = list(set(text)) # Last, just get the set of these. Ignore counts (we are just looking at whether a term existed\n",
    "                            # or not on the website)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = text_cleaner('https://www.indeed.com/viewjob?jk=d4906e2c17725bc2&tk=1easqk7ea2vih000&from=serp&vjs=3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['findings',\n",
       " 'experience',\n",
       " 'solutions',\n",
       " 'technologies',\n",
       " 'work',\n",
       " 'complexity',\n",
       " 'center',\n",
       " 'ux/ui',\n",
       " 'through',\n",
       " 'drinks,',\n",
       " 'testing',\n",
       " 'jobsbrowse',\n",
       " 'problems',\n",
       " 'generation',\n",
       " 'welcomes',\n",
       " 'york.',\n",
       " 'document',\n",
       " 'to',\n",
       " 'user-centered',\n",
       " 'personalized',\n",
       " 'researcher',\n",
       " 'a/b',\n",
       " 'solve',\n",
       " 'excellent',\n",
       " 'sprint',\n",
       " 'candidates',\n",
       " 'save',\n",
       " 'conduct',\n",
       " 'agile',\n",
       " 'way',\n",
       " 'basis',\n",
       " 'updates',\n",
       " 'benefits',\n",
       " 'people',\n",
       " 'love',\n",
       " 'vitamins.',\n",
       " 'we',\n",
       " 'paths',\n",
       " 'customers.',\n",
       " 'surveys,',\n",
       " 'teams',\n",
       " 'tasks',\n",
       " 'team.',\n",
       " 'searchsenior',\n",
       " 'across',\n",
       " 'informationprivacy',\n",
       " 'ambiguous',\n",
       " 'marketers',\n",
       " 'easily',\n",
       " 'original',\n",
       " 'part',\n",
       " 'gain',\n",
       " 'your',\n",
       " 'advocate',\n",
       " 'decisions',\n",
       " 'fair',\n",
       " 'fruits',\n",
       " 'complex',\n",
       " 'in',\n",
       " 'by',\n",
       " 'get',\n",
       " 'combining',\n",
       " 'opportunity',\n",
       " 'grow',\n",
       " 'over',\n",
       " 'team,',\n",
       " 'that',\n",
       " 'sign',\n",
       " 'group',\n",
       " 'or',\n",
       " '5+',\n",
       " 'search',\n",
       " 'remote,',\n",
       " 'experiment',\n",
       " 'ny',\n",
       " 'researcher,',\n",
       " 'starts',\n",
       " 'prospective',\n",
       " 'management',\n",
       " 'multivariate',\n",
       " 'years',\n",
       " 'facilitation',\n",
       " 'berlin',\n",
       " 'usability',\n",
       " 'brainstorming',\n",
       " '\\x99ll',\n",
       " 'companiessalariesfind',\n",
       " 'hands-on',\n",
       " 'are',\n",
       " 'practice',\n",
       " 'short',\n",
       " 'this',\n",
       " 'lot',\n",
       " 'flat',\n",
       " '\\x99re',\n",
       " 'regular',\n",
       " 'a',\n",
       " 'champion',\n",
       " '2020',\n",
       " 'achieve',\n",
       " 'create',\n",
       " 'on',\n",
       " 'conducting',\n",
       " 'inclusive',\n",
       " 'indeeddo',\n",
       " 'researcherhundred-brooklyn,',\n",
       " 'resumecreate',\n",
       " 'in-depth',\n",
       " 'passionate',\n",
       " '25+',\n",
       " 'brand',\n",
       " '1',\n",
       " 'foundation',\n",
       " 'made',\n",
       " 'help',\n",
       " 'what',\n",
       " 'nycompany',\n",
       " 'waiting',\n",
       " 'both',\n",
       " 'ongoing',\n",
       " 'various',\n",
       " 'indeedupload',\n",
       " 'thinking',\n",
       " '(i.e.',\n",
       " 'jobsadvanced',\n",
       " 'hierarchies',\n",
       " 'interviews',\n",
       " 'indeed.com',\n",
       " 'ago',\n",
       " 'centercookies,',\n",
       " 'next',\n",
       " 'change',\n",
       " 'architecture,',\n",
       " 'principles,',\n",
       " 'users',\n",
       " 'ambitious',\n",
       " 'jobapply',\n",
       " 'methods',\n",
       " '11201hundred',\n",
       " 'empathetic,',\n",
       " 'scratch.',\n",
       " 'our',\n",
       " 'best',\n",
       " 'prioritized',\n",
       " 'time,',\n",
       " 'youupload',\n",
       " 'guiding',\n",
       " 'find',\n",
       " 'understanding',\n",
       " 'other',\n",
       " 'recommendations',\n",
       " 'events',\n",
       " 'projects',\n",
       " 'incorporating',\n",
       " 'sitesave',\n",
       " '(preferred)',\n",
       " 'tangible',\n",
       " 'jobscompany',\n",
       " 'intuitive',\n",
       " 'use',\n",
       " 'doing,',\n",
       " 'dynamic',\n",
       " 'be',\n",
       " 'app',\n",
       " 'embrace',\n",
       " 'equal',\n",
       " 'not',\n",
       " 'resume',\n",
       " 'ux',\n",
       " 'gathered',\n",
       " 'will',\n",
       " 'based',\n",
       " 'taking',\n",
       " 'methodologies',\n",
       " 'future',\n",
       " 'creative',\n",
       " 'believe',\n",
       " 'salaries',\n",
       " 'my',\n",
       " 'labcareer',\n",
       " 'diversity.',\n",
       " 'experienced',\n",
       " 'eventswork',\n",
       " 'senior',\n",
       " 'communication,',\n",
       " 'supplements',\n",
       " 'research',\n",
       " 'but',\n",
       " 'privacy',\n",
       " 'focus',\n",
       " 'shape',\n",
       " 'referenced',\n",
       " 'ideas',\n",
       " 'delivering',\n",
       " 'at',\n",
       " 'for',\n",
       " 'learnings',\n",
       " 'you',\n",
       " 'right',\n",
       " 'user',\n",
       " 'also,',\n",
       " 'demonstrates',\n",
       " 'crazy',\n",
       " 'is',\n",
       " 'day',\n",
       " 'a/b,',\n",
       " 'web',\n",
       " 'world.',\n",
       " 'sell',\n",
       " 'environment.',\n",
       " 'research,',\n",
       " 'goals',\n",
       " 'ourselves',\n",
       " 'still',\n",
       " 'new',\n",
       " 'of',\n",
       " 'as',\n",
       " 'project',\n",
       " 'young',\n",
       " 'about',\n",
       " 'personal',\n",
       " 'information',\n",
       " 'startup,',\n",
       " 'resumehiring',\n",
       " 'analytical',\n",
       " 'hundred',\n",
       " 'integrate',\n",
       " 'health',\n",
       " 'jobshare',\n",
       " 'collaborative,',\n",
       " 'decision',\n",
       " 'etc.',\n",
       " 'wireframing,',\n",
       " 'relevant',\n",
       " 'marketing',\n",
       " 'assist',\n",
       " 'them',\n",
       " 'brooklyn,',\n",
       " 'there',\n",
       " 'solid',\n",
       " 'pioneering',\n",
       " 'account',\n",
       " 'employers',\n",
       " 'work.',\n",
       " '-',\n",
       " 'testing),',\n",
       " 'tackling',\n",
       " 'designers',\n",
       " 'nyuser',\n",
       " 'environment',\n",
       " 'strive',\n",
       " 'great',\n",
       " 'tools,',\n",
       " 'design',\n",
       " 'skills',\n",
       " 'their',\n",
       " 'inspiring',\n",
       " 'portfolio',\n",
       " 'youthousands',\n",
       " 'infofollowget',\n",
       " 'groups,',\n",
       " 'free',\n",
       " 'opportunities',\n",
       " 'things',\n",
       " 'problems,',\n",
       " 'company',\n",
       " 'can',\n",
       " 'organizational,',\n",
       " '/',\n",
       " 'nyjobs',\n",
       " 'with',\n",
       " 'know',\n",
       " 'contribute',\n",
       " 'spirit',\n",
       " 'truly',\n",
       " 'qualitative',\n",
       " 'on-site,',\n",
       " 'jobreport',\n",
       " 'build',\n",
       " 'insights',\n",
       " 'consumer',\n",
       " 'trends',\n",
       " 'closely',\n",
       " 'innovative',\n",
       " 'job',\n",
       " 'certificationsindeed',\n",
       " 'reviewsfind',\n",
       " 'click',\n",
       " 'hundredlet',\n",
       " 'jobuser',\n",
       " 'quantitative',\n",
       " 'analytics',\n",
       " 'from',\n",
       " 'open-minded,',\n",
       " 'pride',\n",
       " 'applied',\n",
       " 'share',\n",
       " 'culture.hundred',\n",
       " 'indeedcountriesabouthelp',\n",
       " 'nowapply',\n",
       " 'bring',\n",
       " 'expertise',\n",
       " 'post',\n",
       " 'termslet',\n",
       " 'testing,',\n",
       " 'the',\n",
       " 'jobs',\n",
       " '11201',\n",
       " 'and',\n",
       " 'you!',\n",
       " 'contemporary',\n",
       " 'team',\n",
       " 'in-field',\n",
       " 'an',\n",
       " 'in|employers',\n",
       " 'track',\n",
       " 'so',\n",
       " 'salariesupload',\n",
       " 'employer,',\n",
       " 'such',\n",
       " 'customers',\n",
       " 'whatwherefind',\n",
       " 'current',\n",
       " 'product',\n",
       " 'advicebrowse',\n",
       " 'grasp',\n",
       " 'requirements',\n",
       " 'all',\n",
       " 'organization',\n",
       " 'level.',\n",
       " 'international',\n",
       " 'which']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skills_info(city = None, state = None):\n",
    "    '''\n",
    "    This function will take a desired city/state and look for all new job postings\n",
    "    on Indeed.com. It will crawl all of the job postings and keep track of how many\n",
    "    use a preset list of typical data science skills. The final percentage for each skill\n",
    "    is then displayed at the end of the collation. \n",
    "        \n",
    "    Inputs: The location's city and state. These are optional. If no city/state is input, \n",
    "    the function will assume a national search (this can take a while!!!).\n",
    "    Input the city/state as strings, such as skills_info('Chicago', 'IL').\n",
    "    Use a two letter abbreviation for the state.\n",
    "    \n",
    "    Output: A bar chart showing the most commonly desired skills in the job market for \n",
    "    a data scientist. \n",
    "    '''\n",
    "        \n",
    "    final_job = 'product manager' # searching for data scientist exact fit(\"data scientist\" on Indeed search)\n",
    "    \n",
    "    # Make sure the city specified works properly if it has more than one word (such as San Francisco)\n",
    "    if city is not None:\n",
    "        final_city = city.split() \n",
    "        final_city = '+'.join(word for word in final_city)\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=', final_job, '&l=', final_city,\n",
    "                    '%2C+', state] # Join all of our strings together so that indeed will search correctly\n",
    "    else:\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=', final_job]\n",
    "\n",
    "    final_site = ''.join(final_site_list) # Merge the html address together into one string\n",
    "\n",
    "    \n",
    "    base_url = 'http://www.indeed.com'\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        html = requests.get(final_site) # Open up the front page of our search first\n",
    "    except:\n",
    "        print('That city/state combination did not have any jobs. Exiting . . .') # In case the city is invalid\n",
    "        return\n",
    "    soup = BeautifulSoup(html.text, 'html.parser') # Get the html from the first page\n",
    "    \n",
    "    # Now find out how many jobs there were\n",
    "    \n",
    "    num_jobs_area = soup.find(id = 'searchCountPages').encode('utf-8').decode('unicode_escape') # Now extract the total number of jobs found\n",
    "                                                                        # The 'searchCount' object has this\n",
    "    total_num_jobs = re.findall('of\\s(\\d+[,?\\d+]+)', num_jobs_area)[0] # Extract the total jobs found from the search result\n",
    "    \n",
    "    if len(total_num_jobs) > 3: # Have a total number of jobs greater than or equal to 1000\n",
    "        total_num_jobs = int(float(total_num_jobs.replace(',','')))\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    city_title = city\n",
    "    if city is None:\n",
    "        city_title = 'Nationwide'\n",
    "        state = 'USA'\n",
    "        \n",
    "    print('There were', total_num_jobs, 'jobs found in', city_title + ',' + state) # Display how many jobs were found\n",
    "    \n",
    "    num_pages = int((int(total_num_jobs)/15)) # This will be how we know the number of times we need to iterate over each new\n",
    "                                      # search result page\n",
    "    job_descriptions = [] # Store all our descriptions in this list\n",
    "    \n",
    "    for i in range(0,num_pages+1): # Loop through all of our search result pages\n",
    "        print('Getting page', i+1)\n",
    "        start_num = str(i*10) # Assign the multiplier of 10 to view the pages we want\n",
    "        current_page = ''.join([final_site, '&start=', start_num])\n",
    "        # Now that we can view the correct 10 job returns, start collecting the text samples from each\n",
    "            \n",
    "        html_page = requests.get(current_page) # Get the page\n",
    "        soup = BeautifulSoup(html_page.text, 'html.parser')\n",
    "        \n",
    "        page_obj = soup # Locate all of the job links\n",
    "        job_link_area = page_obj.find(id = \"resultsCol\") # The center column on the page where the job postings exist\n",
    "        \n",
    "        job_URLS = [base_url + link.get('href') for link in job_link_area.find_all('a', target = '_blank') if link.get('href') is not None] # Get the URLS for the jobs\n",
    "            \n",
    "        job_URLS = list(filter(lambda x:'clk' in x, job_URLS)) # Now get just the job related URLS\n",
    "        print(len(job_URLS))\n",
    "        for j in range(0,len(job_URLS)):\n",
    "            final_description = text_cleaner(job_URLS[j])\n",
    "            \n",
    "            if final_description: # So that we only append when the website was accessed correctly\n",
    "                job_descriptions.append(final_description)\n",
    "            \n",
    "    print('Done with collecting the job postings!')    \n",
    "    print('There were', len(job_descriptions), 'jobs successfully found.')\n",
    "    \n",
    "    \n",
    "    doc_frequency = Counter() # This will create a full counter of our terms. \n",
    "    [doc_frequency.update(item) for item in job_descriptions] # List comp\n",
    "    \n",
    "    # Now we can just look at our final dict list inside doc_frequency\n",
    "    \n",
    "    # Obtain our key terms and store them in a dict. These are the key data science skills we are looking for\n",
    "    \n",
    "    prog_lang_dict = Counter({'R':doc_frequency['r'], 'Python':doc_frequency['python'],\n",
    "                    'Java':doc_frequency['java'], 'C++':doc_frequency['c++'],\n",
    "                    'Ruby':doc_frequency['ruby'],\n",
    "                    'Perl':doc_frequency['perl'], 'Matlab':doc_frequency['matlab'],\n",
    "                    'JavaScript':doc_frequency['javascript'], 'Scala': doc_frequency['scala']})\n",
    "                      \n",
    "    analysis_tool_dict = Counter({'Excel':doc_frequency['excel'],  'Tableau':doc_frequency['tableau'],\n",
    "                        'D3.js':doc_frequency['d3.js'], 'SAS':doc_frequency['sas'],\n",
    "                        'SPSS':doc_frequency['spss'], 'D3':doc_frequency['d3']})  \n",
    "\n",
    "    hadoop_dict = Counter({'Hadoop':doc_frequency['hadoop'], 'MapReduce':doc_frequency['mapreduce'],\n",
    "                'Spark':doc_frequency['spark'], 'Pig':doc_frequency['pig'],\n",
    "                'Hive':doc_frequency['hive'], 'Shark':doc_frequency['shark'],\n",
    "                'Oozie':doc_frequency['oozie'], 'ZooKeeper':doc_frequency['zookeeper'],\n",
    "                'Flume':doc_frequency['flume'], 'Mahout':doc_frequency['mahout']})\n",
    "                \n",
    "    database_dict = Counter({'SQL':doc_frequency['sql'], 'NoSQL':doc_frequency['nosql'],\n",
    "                    'HBase':doc_frequency['hbase'], 'Cassandra':doc_frequency['cassandra'],\n",
    "                    'MongoDB':doc_frequency['mongodb']})\n",
    "    \n",
    "    business_dict = Counter({'Collaboration':doc_frequency['collaboration'], 'Strategy':doc_frequency['strategy'],\n",
    "                    'Invision':doc_frequency['invision'], 'Design':doc_frequency['design'],\n",
    "                    'Creativity':doc_frequency['creativity']})\n",
    "                     \n",
    "               \n",
    "    overall_total_skills = prog_lang_dict + analysis_tool_dict + hadoop_dict + database_dict + business_dict # Combine our Counter objects\n",
    "    \n",
    "        \n",
    "    \n",
    "    final_frame = pd.DataFrame(overall_total_skills.items(), columns = ['Skill', 'NumPostings']) # Convert these terms to a \n",
    "                                                                                                # dataframe \n",
    "    \n",
    "    # Change the values to reflect a percentage of the postings \n",
    "    \n",
    "    final_frame.NumPostings = (final_frame.NumPostings)*100/len(job_descriptions) # Gives percentage of job postings \n",
    "                                                                                    #  having that term \n",
    "    \n",
    "    # Sort the data for plotting purposes\n",
    "    \n",
    "    final_frame.sort_values(by = ['NumPostings'], ascending = False, inplace = True)\n",
    "    \n",
    "    # Get it ready for a bar plot\n",
    "        \n",
    "    final_plot = final_frame.plot(x = 'Skill', kind = 'bar', legend = None, rot = 45, figsize = (20,5), \n",
    "                            title = 'Percentage of ' + final_job.title() + ' Job Ads by Key Skill, ' + city_title)\n",
    "        \n",
    "    final_plot.set_ylabel('Percentage Appearing in Job Ads')\n",
    "    fig = final_plot.get_figure() # Have to convert the pandas plot object to a matplotlib object\n",
    "        \n",
    "        \n",
    "    return fig, final_frame # End of the function\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "619\n",
      "There were 619 jobs found in New York,NY\n",
      "Getting page 1\n",
      "13\n",
      "Getting page 2\n",
      "17\n",
      "Getting page 3\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "skills_info(city = 'New York', state = 'NY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
